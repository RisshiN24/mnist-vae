This project implements a Variational Autoencoder (VAE) trained on the MNIST handwritten digit dataset using TensorFlow and Keras. The model is built using a modular, object-oriented design with custom training logic and support for tuning the KL divergence weight (Î²). It currently uses fully connected layers for the encoder and decoder, and supports both binary crossentropy and mean squared error as reconstruction loss functions. The goal is to learn a compact latent representation of digits and reconstruct them accurately. Future updates will include a convolutional architecture for improved image quality.
